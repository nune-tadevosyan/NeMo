# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import torch
from torch import nn as nn
from torch.nn import LayerNorm

from nemo.collections.asr.parts.submodules.adapters.attention_adapter_mixin import AttentionAdapterModuleMixin
from nemo.collections.asr.parts.submodules.batchnorm import FusedBatchNorm1d
from nemo.collections.asr.parts.submodules.causal_convs import CausalConv1D
from nemo.collections.asr.parts.submodules.multi_head_attention import (
    MultiHeadAttention,
    RelPositionMultiHeadAttention,
    RelPositionMultiHeadAttentionLongformer,
)
from nemo.collections.asr.parts.utils.activations import Swish
from nemo.collections.common.parts.utils import activation_registry
from nemo.core.classes.mixins import AccessMixin
from mamba_ssm.models.mixer_seq_simple import create_ssmblock
from nemo.collections.asr.parts.submodules.conformer_modules import ConformerConvolution, ConformerFeedForward

__all__ = ['MambaAttentionLayer']


class MambaAttentionLayer(torch.nn.Module, AttentionAdapterModuleMixin, AccessMixin):
    """A single block of the Conformer encoder.

    Args:
        d_model (int): input dimension of MultiheadAttentionMechanism and PositionwiseFeedForward
        d_ff (int): hidden dimension of PositionwiseFeedForward
        self_attention_model (str): type of the attention layer and positional encoding
            'rel_pos': relative positional embedding and Transformer-XL
            'rel_pos_local_attn': relative positional embedding and Transformer-XL with local attention using
                overlapping chunks. Attention context is determined by att_context_size parameter.
            'abs_pos': absolute positional embedding and Transformer
            Default is rel_pos.
        global_tokens (int): number of tokens to be used for global attention.
            Only relevant if self_attention_model is 'rel_pos_local_attn'.
            Defaults to 0.
        global_tokens_spacing (int): how far apart the global tokens are
            Defaults to 1.
        global_attn_separate (bool): whether the q, k, v layers used for global tokens should be separate.
            Defaults to False.
        n_heads (int): number of heads for multi-head attention
        conv_kernel_size (int): kernel size for depthwise convolution in convolution module
        dropout (float): dropout probabilities for linear layers
        dropout_att (float): dropout probabilities for attention distributions
        use_bias (bool): Apply bias to all Linear and Conv1d layers from each ConformerLayer to improve activation flow and stabilize training of huge models.
            Defaults to True.
"""

    def __init__(
        self,
        d_model,
        d_ff,
        add_self_attention,
        self_attention_model='rel_pos',
        global_tokens=0,
        global_tokens_spacing=1,
        global_attn_separate=False,
        n_heads=4,
        expand=2,
        d_state=16,
        d_conv=4,      
        causal = True,  
        mamba_vision = True,        
        gated_mlp = True,
        mlp_ratio=2,
        conv_kernel_size=31,
        conv_norm_type='batch_norm',
        conv_context_size=None,
        dropout=0.1,
        dropout_att=0.1,
        pos_bias_u=None,
        pos_bias_v=None,
        att_context_size=[-1, -1],
        use_bias=True,
    ):
        super(MambaAttentionLayer, self).__init__()
        self.add_self_attention = add_self_attention

        self.self_attention_model = self_attention_model
        self.n_heads = n_heads
        self.fc_factor = 0.5

        # first feed forward module
        self.norm_feed_forward1 = LayerNorm(d_model)
        self.feed_forward1 = ConformerFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout, use_bias=use_bias)

        # convolution module
        self.norm_conv = LayerNorm(d_model)
        self.conv = ConformerConvolution(
            d_model=d_model,
            kernel_size=conv_kernel_size,
            norm_type=conv_norm_type,
            conv_context_size=conv_context_size,
            use_bias=use_bias,
        )

        # multi-headed self-attention module
        if self.add_self_attention:
            self.norm_self_att = LayerNorm(d_model)
            MHA_max_cache_len = att_context_size[0]

        if self.add_self_attention:
            if self_attention_model == 'rel_pos':
                self.self_attn = RelPositionMultiHeadAttention(
                    n_head=n_heads,
                    n_feat=d_model,
                    dropout_rate=dropout_att,
                    pos_bias_u=pos_bias_u,
                    pos_bias_v=pos_bias_v,
                    max_cache_len=MHA_max_cache_len,
                    use_bias=use_bias,
                )
            elif self_attention_model == 'rel_pos_local_attn':
                self.self_attn = RelPositionMultiHeadAttentionLongformer(
                    n_head=n_heads,
                    n_feat=d_model,
                    dropout_rate=dropout_att,
                    pos_bias_u=pos_bias_u,
                    pos_bias_v=pos_bias_v,
                    max_cache_len=MHA_max_cache_len,
                    att_context_size=att_context_size,
                    global_tokens=global_tokens,
                    global_tokens_spacing=global_tokens_spacing,
                    global_attn_separate=global_attn_separate,
                    use_bias=use_bias,
                )
            elif self_attention_model == 'abs_pos':
                self.self_attn = MultiHeadAttention(
                    n_head=n_heads,
                    n_feat=d_model,
                    dropout_rate=dropout_att,
                    max_cache_len=MHA_max_cache_len,
                    use_bias=use_bias,
                )
            else:
                raise ValueError(
                    f"'{self_attention_model}' is not not a valid value for 'self_attention_model', "
                    f"valid values can be from ['rel_pos', 'rel_pos_local_attn', 'abs_pos']"
                )

        # second feed forward module
        self.norm_feed_forward2 = LayerNorm(d_model)
        self.feed_forward2 = ConformerFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout, use_bias=use_bias)

        self.dropout = nn.Dropout(dropout)
        self.norm_out = LayerNorm(d_model)
        model_name = "MambaVision" if mamba_vision else "Mamba1"
        ssm_cfg = {"expand": expand, "d_state": d_state, "d_conv": d_conv, "layer": model_name, "causal": causal}
        attn_cfg ={"num_heads": n_heads}
        d_intermediate = 0
        #d_model
        self.attn_layer_idx = {}        
        initializer_cfg = None
        self.mamba = create_ssmblock(
                    d_model,
                    d_intermediate,
                    attn_layer_idx=self.attn_layer_idx,
                    layer_idx=0,
                    ssm_cfg=ssm_cfg, 
                    attn_cfg=attn_cfg,                   
                    residual_in_fp32=False,
                    gated_mlp = gated_mlp,
                    mlp_ratio = mlp_ratio,
                    fc_factor  = 1,
                    rms_norm=False,
                )                 

    def forward(self, x, att_mask=None, pos_emb=None, pad_mask=None, cache_last_channel=None, cache_last_time=None, cache_ssm=None, cache_conv=None):
        """
        Args:
            x (torch.Tensor): input signals (B, T, d_model)
            att_mask (torch.Tensor): attention masks(B, T, T)
            pos_emb (torch.Tensor): (L, 1, d_model)
            pad_mask (torch.tensor): padding mask
            cache_last_channel (torch.tensor) : cache for MHA layers (B, T_cache, d_model)
            cache_last_time (torch.tensor) : cache for convolutional layers (B, d_model, T_cache)
        Returns:
            x (torch.Tensor): (B, T, d_model)
            cache_last_channel (torch.tensor) : next cache for MHA layers (B, T_cache, d_model)
            cache_last_time (torch.tensor) : next cache for convolutional layers (B, d_model, T_cache)
        """
        residual = x
        x = self.norm_feed_forward1(x)
        x = self.feed_forward1(x)
        residual = residual + self.dropout(x) * self.fc_factor
        residual, cache_ssm, cache_conv =  self.mamba(residual, cache_ssm=cache_ssm, cache_conv=cache_conv)
        if self.add_self_attention:
            if self.add_self_attention:
                x = self.norm_self_att(residual)
                if self.self_attention_model == 'rel_pos':
                    x = self.self_attn(query=x, key=x, value=x, mask=att_mask, pos_emb=pos_emb, cache=cache_last_channel)
                elif self.self_attention_model == 'rel_pos_local_attn':
                    x = self.self_attn(query=x, key=x, value=x, pad_mask=pad_mask, pos_emb=pos_emb, cache=cache_last_channel)
                elif self.self_attention_model == 'abs_pos':
                    x = self.self_attn(query=x, key=x, value=x, mask=att_mask, cache=cache_last_channel)
                else:
                    x = None

        if x is not None and cache_last_channel is not None:
            (x, cache_last_channel) = x

        residual = residual + self.dropout(x)


        x = self.norm_conv(residual)
        x = self.conv(x, pad_mask=pad_mask, cache=cache_last_time)
        if cache_last_time is not None:
            (x, cache_last_time) = x
        residual = residual + self.dropout(x)

        x = self.norm_feed_forward2(residual)
        x = self.feed_forward2(x)
        residual = residual + self.dropout(x) * self.fc_factor

        x = self.norm_out(residual)


        if self.is_access_enabled(getattr(self, "model_guid", None)) and self.access_cfg.get(
            'save_encoder_tensors', False
        ):
            self.register_accessible_tensor(name='encoder', tensor=x)
        if cache_last_channel is None:
            return x
        else:
            return x, cache_last_channel, cache_last_time, cache_ssm, cache_conv


